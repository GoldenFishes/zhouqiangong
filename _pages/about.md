---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}


<span class='anchor' id='about_me'></span>
# About Me
Hello and welcome to my homepage!

I was born in 2002 and am one of the many newcomers in the field of Artificial Intelligence.
My primary research interests lie in Computer Vision, but I am also passionate about integrating multimodal learning to tackle real-world problems.

I aim to simplify complex challenges by optimizing how problems are defined, and Iâ€™m committed to deepening my theoretical understanding of deep learning to gain broader perspectives.

Iâ€™m always open to collaboration in any form â€” feel free to reach out to me via email at 2021252439@qq.com


<span class='anchor' id='educations'></span>
# Educations
- *2020.09 - 2024.06*, Bachelor of Science. in Data Science and Big Data Technology, Beijing Institute of Technology, Zhuhai, China.


<span class='anchor' id='skills'></span>
# Skills
ðŸ”¹ Programming Languages:
Experienced in Python development; have a basic understanding of Java, C++, and R.

ðŸ”¹ Development Environment:
Experienced in Linux development; comfortable with terminal-based workflows and scripting.

ðŸ”¹ Deep Learning Frameworks:
Familiar with mainstream frameworks such as PyTorch; well-versed in popular algorithms in Computer Vision (CV) and Natural Language Processing (NLP).

ðŸ”¹ Model Lifecycle:
Skilled in the full pipeline of model development, including design, training, lightweight optimization (e.g., quantization, pruning), and deployment.

ðŸ”¹ Machine Learning Methods:
Solid understanding of machine learning, deep learning, and reinforcement learning techniques.

ðŸ”¹ Data Handling:
Capable of building web crawlers, performing data cleaning, analysis, and visualization.


<span class='anchor' id='publications'></span>
# Projects

<div class='paper-box' style="width: 100%; margin-bottom: 30px;">
  <div class='paper-box-image' style="width: 100%; display: flex; justify-content: flex-start; margin-bottom: 15px; overflow-x: auto;">
    <img src='images/Allen_AgentAction.jpg' alt="AgentAction" style="max-height: 200px; object-fit: scale-down; margin-right: 10px;">
    <img src='images/MAS.jpg' alt="MAS" style="max-height: 200px; object-fit: scale-down; margin-right: 10px;">
  </div>

  <div class='paper-box-text' markdown="1" style="width: 100%;">
  **Allen:Rethinking MAS Design through Step-Level Policy Autonomy**

  **Qiangong Zhou**, Zhiting Wang, Mingyou Yao, Zongyang Liu

  [**Project**](https://github.com/motern88/Allen) <strong><span class='show_paper_citations' data='AunSnE4AAAAJ:9yKSN-GCB0IC'></span></strong>

  We propose a novel Multi-Agent System (MAS) architecture that redefines the Step as the smallest unit within the MAS, 
  enabling agents to autonomously determine their own working modes and achieving unprecedented policy autonomy.
  In addition, we make trade-offs between agent collaboration structure and controllability.

  **Key words**: Multi-Agent System
  </div>
</div>

<div class='paper-box' style="width: 100%; margin-bottom: 30px;">
  <div class='paper-box-image' style="width: 100%; display: flex; margin-bottom: 15px;">
    <img src='images/Det-SAM2.jpg' alt="Det-SAM2" style="width: 100%; max-height: 200px; object-fit: contain;">
  </div>

  <div class='paper-box-text' markdown="1" style="width: 100%;">
  **Det-SAM2:Technical Report on the Self-Prompting Segmentation Framework Based on Segment Anything Model 2**

  Zhiting Wang, **Qiangong Zhou**, Zongyang Liu

  [**Tech Report**](https://arxiv.org/abs/2411.18977)  [**Project**](https://github.com/motern88/Det-SAM2) <strong><span class='show_paper_citations' data='AunSnE4AAAAJ:9yKSN-GCB0IC'></span></strong>

  We developed a real-time video stream inference pipeline based on SAM2, which integrates an object detection model to automatically provide conditional prompts for long video segmentation.
  This is the first fully functional self-prompted SAM2 framework for long videos. It supports advanced features such as online addition of new categories during inference and preloading a memory bank.
  Furthermore, the pipeline maintains constant GPU and CPU memory usage throughout, enabling efficient inference on arbitrarily long videos.

  **Key words**: Video Segmentation, SAM2
  </div>
</div>


<div class='paper-box' style="width: 100%; margin-bottom: 30px;">
  <div class='paper-box-image' style="width: 100%; display: flex; justify-content: flex-start; margin-bottom: 15px; overflow-x: auto;">
    <img src='images/HiLight.jpg' alt="HiLight" style="max-height: 200px; object-fit: scale-down; margin-right: 10px;">
    <img src='images/CLIP-ViP-PLUS.jpg' alt="CLIP-ViP-PLUS" style="max-height: 200px; object-fit: scale-down; margin-right: 10px;">
  </div>

  <div class='paper-box-text' markdown="1" style="width: 100%;">
  **HiLight: Technical Report on the Motern AI Video Language Model**

  Zhiting Wang, **Qiangong Zhou**, Kangjie Yang, Zongyang Liu, Xin Mao

  [**Tech Report**](https://arxiv.org/abs/2407.07325)  [**Project**](https://github.com/motern88/HiLight) <strong><span class='show_paper_citations' data='AunSnE4AAAAJ:d1gkVwhDpl0C'></span></strong>

  We first design a VideoEncoder with finer-grained modality alignment by introducing a contrastive loss between patches and tokens based on CLIP-ViP.
  Then, we build a dual-tower vision encoder composed of the improved CLIP-ViP and Long-CLIP, which extracts both video and image features.
  The fused visual features are fed into the Gemma-2B language model, enabling video-based dialogue capability.

  **Key words**: Video Language Model, CLIP-ViP, Long-CLIP, Gemma-2B
  </div>
</div>


<div class='paper-box' style="width: 100%; margin-bottom: 30px;">
  <div class='paper-box-image' style="width: 100%; display: flex; margin-bottom: 15px;">
    <img src='images/FreeV.jpg' alt="FreeV" style="width: 100%; max-height: 200px; object-fit: contain;">
  </div>

  <div class='paper-box-text' markdown="1" style="width: 100%;">
  **FreeV:Free Lunch in MultiModal Diffusion U-ViT**

  **Qiangong Zhou**, Youyu Zhou ,Yahong Wang

  [**Paper**](https://www.authorea.com/doi/full/10.36227/techrxiv.24633840.v1)  [**Project**](https://github.com/GoldenFishes/FreeV) <strong><span class='show_paper_citations' data='AunSnE4AAAAJ:u-x6o8ySG0sC'></span></strong>

  We propose FreeV, an adaptation of the FreeU strategyâ€”originally designed for U-Netâ€”into the Transformer-based U-ViT architecture.
  FreeV significantly improves generation quality without additional training or fine-tuning.
  The key insight is to balance the contributions from the backbone, skip connections, and fused features within U-ViT, maximizing its strengths while addressing its limitations in feature fusion.

  **Key words**: U-ViT, Diffusion Model
  </div>
</div>


<div class='paper-box' style="width: 100%; margin-bottom: 30px;">
  <div class='paper-box-image' style="width: 100%; display: flex; margin-bottom: 15px;">
    <img src='images/Optimal_Use_of_Attention_Mechanisms.jpg' alt="Optimal_Use_of_Attention_Mechanisms" style="width: 100%; max-height: 200px; object-fit: contain;">
  </div>
  <div class='paper-box-text' markdown="1" style="width: 100%;">
  **Optimal Use of Attention Mechanisms: Comparative Study in U-Net for Image Segmentation Tasks**

  **Qiangong Zhou**, Guanzhang Su, Jiayi Chen, Yuangen Chen, Youyu Zhou

  [**Paper**](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13063/130630L/Optimal-use-of-attention-mechanisms--comparative-study-in-U/10.1117/12.3021498.short) <strong><span class='show_paper_citations' data='AunSnE4AAAAJ:u5HHmVD_uO8C'></span></strong>

  The study finds that while attention can enhance channel-wise feature weighting, it may negatively affect the backbone's convolutional feature extraction.
  However, it effectively complements the encoder-decoder structure by improving high-level semantic representation.

  **Key words**: Attention Mechanism, U-Net, Image Segmentation
  </div>
</div>


<span class='anchor' id='cv'></span>
# Curriculum Vitae
You can download my chinese CV in PDF format from the following link:
<a href="https://github.com/GoldenFishes/zhouqiangong/raw/main/docs/ZhouQianGong_CV.pdf" download="ZhouQianGong_CV_Chinese.pdf">Download CV</a>
<!-- å°†githubé“¾æŽ¥ä¸­çš„blobæ”¹ä¸ºrawå³å¯å½¢æˆä¸‹è½½é“¾æŽ¥ -->
